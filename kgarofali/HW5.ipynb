{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "A script to compare different root-finding algorithms.\n",
      "\n",
      "This version of the script is buggy and does not execute. It is your task\n",
      "to find an fix these bugs.\n",
      "\n",
      "The output of the script sould look like:\n",
      "\n",
      "    Benching 1D root-finder optimizers from scipy.optimize:\n",
      "                brenth:   604678 total function calls\n",
      "                brentq:   594454 total function calls\n",
      "                ridder:   778394 total function calls\n",
      "                bisect:  2148380 total function calls\n",
      "\"\"\"\n",
      "from itertools import product\n",
      "\n",
      "import numpy as np\n",
      "from scipy import optimize\n",
      "\n",
      "FUNCTIONS = (np.tan,  # Dilating map\n",
      "             np.tanh, # Contracting map\n",
      "             lambda x: x**3 + 1e-4*x, # Almost null gradient at the root\n",
      "             lambda x: x+np.sin(2*x), # Non monotonous function\n",
      "             lambda x: 1.1*x+np.sin(4*x), # Fonction with several local maxima\n",
      "            )\n",
      "\n",
      "OPTIMIZERS = (optimize.brenth, optimize.brentq, optimize.ridder,\n",
      "              optimize.bisect)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def apply_optimizer(optimizer, func, a, b):\n",
      "    \"\"\" Return the number of function calls given an root-finding optimizer, \n",
      "        a function and upper and lower bounds.\n",
      "    \"\"\"\n",
      "\n",
      "    return optimizer(func, a, b, full_output=True)[1].function_calls\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def bench_optimizer(optimizer, param_grid):\n",
      "    \"\"\" Find roots for all the functions, and upper and lower bounds\n",
      "        given and return the total number of function calls.\n",
      "    \"\"\"\n",
      "\n",
      "    return sum(apply_optimizer(optimizer, func, a, b) for func, a, b in param_grid)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def compare_optimizers():\n",
      "    \"\"\" Compare all the optimizers given on a grid of a few different\n",
      "        functions all admitting a signle root in zero and a upper and\n",
      "        lower bounds.\n",
      "    \"\"\"\n",
      "    random_a = -1.3 + np.random.random(size=100)\n",
      "    random_b =   .3 + np.random.random(size=100)\n",
      "    \n",
      "    #generate param_grid using list comprehension so that the values are stored after compare_optimizers\n",
      "    #is called. \n",
      "    param_grid = [(tmp1, tmp2, tmp3) for tmp1 in FUNCTIONS for tmp2 in random_a for tmp3 in random_b]\n",
      "    print \"Benching 1D root-finder optimizers from scipy.optimize:\"\n",
      "  \n",
      "   \n",
      "    for optimizer in OPTIMIZERS:\n",
      "        #could also just move param_grid down here \n",
      "        #param_grid = product(FUNCTIONS, random_a, random_b)\n",
      "        print '% 20s: %8i' % (\n",
      "                    optimizer.__name__, \n",
      "                    bench_optimizer(optimizer, param_grid)\n",
      "                    )\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "if __name__ == '__main__':  \n",
      "  \n",
      "    compare_optimizers() \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Benching 1D root-finder optimizers from scipy.optimize:\n",
        "              brenth:   604351"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              brentq:   594071"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              ridder:   773822"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "              bisect:  2148370"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I first noticed that the code would spit out an error about not being able to sum over a tuple, which lead me to remove the spurious comma from the end of the return line in the apply_optimizers function. When I reran the program after this change I noticed that it only output the number of function calls for the first optimizer in the list, which was the next error that I attempted to debug. I could tell that it was looping through the entirety of the OPTIMIZERS list, because all the optimizers were printed out, but none of them after the first optimizer returned the correct values for the number of function calls. I changed the order of the optimizers in the list just to check that the correct number of function calls were being ouput for each, which proved to be true.\n",
      "\n",
      "To figure out where the looping was going awry I added print statements to the apply_optimzers, bench_optimizers, and compare_optimziers functions to see when they were actually being executed. I reduced the size of the param_grid for ease of reading the output of these print statemements. The print statements revealed that the code only ran through the apply_optimizers function for the first optimizer and then was not called for any of the other optimizers (no print statements appeared, and no values of param_grid were printed). This led to me to look up how itertools.product actually functions to create the param_grid. It outputs the cartesian product, and is equivalent to doing nested for loops in a generator expression. Generator expressions, unlike using list comprehension statements, do not create and save an entire list in memory, but instead create the list \"on the fly.\" This means that because compare_optimizers is only called once param_grid is only created once, and does not store values in memory for all the subsequent optimizers. I generated param_grid using a list comprehension instead of using \"product\" and this got the code to ouput the correct values. This is also equivalent to simply moving param_grid inside the for loop within compare_optimizers and using itertools.product. I'm not sure which method is considered better practice. I would assume there is something to be said for not having to store the entire param_grid in memory, but there is also probably cases where it is better to not create the param_grid each time through the for loop.\n",
      "\n",
      "Note: I removed all the print statements I used when debugging because it made the output really annoying to read.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}